# RLAgent ë¡œì§ ê°œì„ ì  ë¶„ì„ ë³´ê³ ì„œ

**ì‘ì„±ì¼:** 2026-01-25
**ë¶„ì„ ëŒ€ìƒ:** `local_training/rl_agent.py` & `bot_step_integration.py`

---

## ğŸ“Š í˜„ì¬ êµ¬ì¡° ìš”ì•½

### ì•Œê³ ë¦¬ì¦˜
- **REINFORCE** (Policy Gradient)
- **ìƒíƒœ ê³µê°„:** 15ì°¨ì› (ìì›, ì„œí”Œë¼ì´, ìœ ë‹› ìˆ˜, ì—…ê·¸ë ˆì´ë“œ, êµ°ëŒ€ HP ë“±)
- **í–‰ë™ ê³µê°„:** 5ê°œ (ECONOMY, AGGRESSIVE, DEFENSIVE, TECH, ALL_IN)
- **ì‹ ê²½ë§:** 3-layer MLP (15 â†’ 64 â†’ 64 â†’ 5)

### í•™ìŠµ íë¦„
1. ë§¤ ìŠ¤í…ë§ˆë‹¤ `get_action()` í˜¸ì¶œ â†’ í™•ë¥ ì  ìƒ˜í”Œë§
2. ë§¤ ìŠ¤í…ë§ˆë‹¤ `update_reward()` í˜¸ì¶œ â†’ ë³´ìƒ ì €ì¥
3. ê²Œì„ ì¢…ë£Œ ì‹œ `end_episode()` í˜¸ì¶œ â†’ ì „ì²´ ì—í”¼ì†Œë“œ í•™ìŠµ
4. ëª¨ë¸ ì €ì¥ (`save_model()`)

---

## ğŸ”´ ì£¼ìš” ë¬¸ì œì 

### 1. **íƒí—˜-í™œìš© ê· í˜• ì—†ìŒ** (ì‹¬ê°)

**ë¬¸ì œ:**
```python
# Line 176 - rl_agent.py
action_idx = np.random.choice(len(probs), p=probs)
```
- **í•­ìƒ** í™•ë¥  ë¶„í¬ì—ì„œ ëœë¤ ìƒ˜í”Œë§
- í•™ìŠµ ì´ˆê¸°: ëœë¤ í–‰ë™ â†’ "Untrained agent causes random behavior"
- í•™ìŠµ í›„ê¸°: ìµœì  í–‰ë™ì„ ì¼ê´€ë˜ê²Œ ì„ íƒí•˜ì§€ ëª»í•¨

**ì˜í–¥:**
- í•™ìŠµë˜ì§€ ì•Šì€ ì´ˆê¸° ëª¨ë¸ì€ ì™„ì „íˆ ëœë¤í•˜ê²Œ í–‰ë™
- ì‚¬ìš©ìê°€ "DISABLED (Untrained)"ë¡œ ë¹„í™œì„±í™”í•œ ì´ìœ 

**ê°œì„  ë°©ì•ˆ:**
```python
# Epsilon-greedy ì „ëµ
def get_action(self, state, epsilon=0.1, training=True):
    probs, cache = self.policy.forward(state)

    if training and np.random.rand() < epsilon:
        # íƒí—˜: ëœë¤ í–‰ë™
        action_idx = np.random.randint(len(probs))
    else:
        # í™œìš©: ìµœì„  í–‰ë™ (ë˜ëŠ” í™•ë¥ ì  ìƒ˜í”Œë§)
        if training:
            action_idx = np.random.choice(len(probs), p=probs)
        else:
            action_idx = np.argmax(probs)  # ì¶”ë¡  ì‹œ greedy

    return action_idx, self.action_labels[action_idx], float(probs[action_idx])
```

---

### 2. **High Variance ë¬¸ì œ** (ì‹¬ê°)

**ë¬¸ì œ:**
- REINFORCE ì•Œê³ ë¦¬ì¦˜ì€ ë³¸ì§ˆì ìœ¼ë¡œ **gradient varianceê°€ ë†’ìŒ**
- Baselineì„ ì‚¬ìš©í•˜ì§€ë§Œ ë‹¨ìˆœ ì´ë™ í‰ê· :
```python
# Line 202 - rl_agent.py
self.baseline = self.baseline_decay * self.baseline + (1 - self.baseline_decay) * avg_return
```
- ê²Œì„ë§ˆë‹¤ ê¸¸ì´ê°€ ë‹¤ë¥´ê³  ë³´ìƒ ë¶„í¬ê°€ ë¶ˆì•ˆì •

**ì˜í–¥:**
- í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ê³  ìˆ˜ë ´ ì†ë„ê°€ ëŠë¦¼
- 100+ ê²Œì„ì„ í•´ë„ ì¢‹ì€ ì •ì±…ì„ ì°¾ì§€ ëª»í•  ìˆ˜ ìˆìŒ

**ê°œì„  ë°©ì•ˆ:**
1. **Value Network ì¶”ê°€ (Actor-Critic)**
   ```python
   class ValueNetwork:
       """ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ V(s) ì˜ˆì¸¡"""
       # ìƒíƒœ â†’ ì˜ˆìƒ ë¦¬í„´ ê°’
   ```

2. **Generalized Advantage Estimation (GAE)**
   ```python
   def calculate_gae(self, rewards, values, gamma=0.99, lambda_=0.95):
       # TD-error ê¸°ë°˜ advantage ê³„ì‚°ìœ¼ë¡œ variance ê°ì†Œ
   ```

---

### 3. **ê²½í—˜ ì¬ì‚¬ìš© ì—†ìŒ** (ì¤‘ê°„)

**ë¬¸ì œ:**
```python
# Line 227 - rl_agent.py
self._clear_buffers()  # í•™ìŠµ í›„ ëª¨ë“  ê²½í—˜ ì‚­ì œ
```
- í•œ ì—í”¼ì†Œë“œë¡œ í•œ ë²ˆë§Œ í•™ìŠµí•˜ê³  ë°ì´í„°ë¥¼ ë²„ë¦¼
- **Sample efficiencyê°€ ë§¤ìš° ë‚®ìŒ**

**í˜„ì¬ ìƒíƒœ:**
- `train_from_batch()` ë©”ì„œë“œëŠ” ìˆì§€ë§Œ **ì‚¬ìš©ë˜ì§€ ì•ŠìŒ**
- `save_experience_data()` ë©”ì„œë“œë„ ìˆì§€ë§Œ **í˜¸ì¶œë˜ì§€ ì•ŠìŒ**

**ê°œì„  ë°©ì•ˆ:**
```python
# 1. ê²½í—˜ ë²„í¼ì— ì €ì¥
class ExperienceReplay:
    def __init__(self, max_size=10000):
        self.buffer = []
        self.max_size = max_size

    def add(self, episode):
        if len(self.buffer) >= self.max_size:
            self.buffer.pop(0)
        self.buffer.append(episode)

    def sample(self, batch_size=32):
        # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ë°°ì¹˜ í•™ìŠµ
        pass

# 2. ì£¼ê¸°ì ìœ¼ë¡œ ë°°ì¹˜ í•™ìŠµ
if len(self.replay_buffer) >= batch_size:
    batch = self.replay_buffer.sample(batch_size)
    self.train_from_batch(batch)
```

---

### 4. **ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ë¬¸ì œ** (ì¤‘ê°„)

**ë¬¸ì œ:**
- ë³´ìƒ ì‹œìŠ¤í…œì€ 11ê°œ ì»´í¬ë„ŒíŠ¸ì˜ í•©:
  ```python
  # reward_system.py
  reward += self._calculate_creep_reward(bot)           # 0~0.1
  reward += self._calculate_larva_efficiency_reward()   # 0~0.1
  reward += self._calculate_resource_turnover_reward()  # -0.2~0.0
  # ... ì´ 11ê°œ
  ```
- ê° ì»´í¬ë„ŒíŠ¸ì˜ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥´ê³ , í•©ì‚° ë²”ìœ„ê°€ ë¶ˆëª…í™•
- Advantage ì •ê·œí™”ê°€ ìˆì§€ë§Œ ë³´ìƒ ìì²´ëŠ” ì •ê·œí™” ì•ˆë¨

**ê°œì„  ë°©ì•ˆ:**
```python
# 1. ë³´ìƒ í´ë¦¬í•‘
reward = np.clip(total_reward, -1.0, 1.0)

# 2. ë³´ìƒ ì •ê·œí™” (running mean/std)
class RunningMeanStd:
    def __init__(self):
        self.mean = 0.0
        self.std = 1.0
        self.count = 0

    def update(self, x):
        # ì˜¨ë¼ì¸ mean/std ì—…ë°ì´íŠ¸
        pass

    def normalize(self, x):
        return (x - self.mean) / (self.std + 1e-8)
```

---

### 5. **ìƒíƒœ ë²¡í„° ì •ê·œí™” ë¶ˆì¼ì¹˜** (ë‚®ìŒ)

**ë¬¸ì œ:**
```python
# bot_step_integration.py:640
game_state = np.array([
    getattr(self.bot, "minerals", 0) / 2000.0,  # ì •ê·œí™”ë¨
    getattr(self.bot, "vespene", 0) / 1000.0,   # ì •ê·œí™”ë¨
    # ...
    map_control,  # ì´ë¯¸ 0~1 ë²”ìœ„
    our_army_hp / 5000.0,  # ì •ê·œí™”ë¨
    enemy_army_hp / 5000.0  # ì •ê·œí™”ë¨
])
```
- ê° featureì˜ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¦„ (ì¼ë¶€ëŠ” 0~1, ì¼ë¶€ëŠ” 0~ë¬´í•œëŒ€ ê°€ëŠ¥)
- ë§¤ì§ ë„˜ë²„ ì‚¬ìš© (2000, 1000, 5000)
- ì‹¤ì œ ìµœëŒ“ê°’ì„ ì´ˆê³¼í•  ìˆ˜ ìˆìŒ (ì˜ˆ: ë¯¸ë„¤ë„ > 2000)

**ê°œì„  ë°©ì•ˆ:**
```python
# StandardScaler ì‚¬ìš©
from sklearn.preprocessing import StandardScaler

class StateNormalizer:
    def __init__(self):
        self.scaler = StandardScaler()
        self.fitted = False

    def normalize(self, state):
        if not self.fitted:
            self.scaler.partial_fit([state])
            self.fitted = True
        return self.scaler.transform([state])[0]
```

---

### 6. **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ì—†ìŒ** (ë‚®ìŒ)

**ë¬¸ì œ:**
```python
# Line 140 - rl_agent.py
self.learning_rate = learning_rate  # ê³ ì • í•™ìŠµë¥  (0.001)
```
- í•™ìŠµ ì´ˆê¸°: í° í•™ìŠµë¥  í•„ìš” (ë¹ ë¥¸ í•™ìŠµ)
- í•™ìŠµ í›„ê¸°: ì‘ì€ í•™ìŠµë¥  í•„ìš” (ì•ˆì •í™”)

**ê°œì„  ë°©ì•ˆ:**
```python
# Cosine Annealing
def get_learning_rate(self, episode, max_episodes=1000):
    min_lr = 1e-5
    max_lr = 1e-3
    return min_lr + 0.5 * (max_lr - min_lr) * (1 + np.cos(np.pi * episode / max_episodes))

# Step Decay
def get_learning_rate(self, episode):
    initial_lr = 1e-3
    decay_rate = 0.95
    decay_steps = 100
    return initial_lr * (decay_rate ** (episode // decay_steps))
```

---

### 7. **ëª¨ë¸ ê²€ì¦ ë¡œì§ ì—†ìŒ** (ì¤‘ê°„)

**ë¬¸ì œ:**
- í•™ìŠµëœ ëª¨ë¸ì´ ì‹¤ì œë¡œ ì˜ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦ ì—†ìŒ
- "Untrained" ìƒíƒœì¸ì§€ íŒë‹¨í•  ê¸°ì¤€ ì—†ìŒ
- í•™ìŠµ ì§„ì²™ë„ ì¶”ì  ë¶ˆê°€

**ê°œì„  ë°©ì•ˆ:**
```python
class RLAgent:
    def __init__(self):
        # ...
        self.validation_scores = []
        self.min_games_for_deployment = 50  # ìµœì†Œ í•™ìŠµ ê²Œì„ ìˆ˜

    def is_ready_for_deployment(self):
        """ë°°í¬ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨"""
        if self.episode_count < self.min_games_for_deployment:
            return False, "Not enough training games"

        if len(self.validation_scores) < 10:
            return False, "Not enough validation games"

        avg_score = np.mean(self.validation_scores[-10:])
        if avg_score < 0.5:  # ì„ê³„ê°’
            return False, f"Validation score too low: {avg_score:.3f}"

        return True, "Model ready"

    def validate(self, game_result):
        """ê²Œì„ ê²°ê³¼ë¡œ ëª¨ë¸ ê²€ì¦"""
        self.validation_scores.append(game_result)
```

---

### 8. **Batch Learning ë¯¸êµ¬í˜„** (ì¤‘ê°„)

**ë¬¸ì œ:**
- `train_from_batch()` ë©”ì„œë“œëŠ” êµ¬í˜„ë˜ì–´ ìˆì§€ë§Œ:
  1. **í˜¸ì¶œë˜ì§€ ì•ŠìŒ**
  2. **ê²½í—˜ ë°ì´í„°ê°€ ì €ì¥ë˜ì§€ ì•ŠìŒ**
  3. **Background Learnerì™€ ì—°ë™ ì•ˆë¨**

**í˜„ì¬ ìƒíƒœ:**
```
[BACKGROUND LEARNER] STATUS REPORT
Active Workers:       0/1  â† ì‘ë™ ì•ˆí•¨
Files Processed:      0    â† ê²½í—˜ ë°ì´í„° ì—†ìŒ
```

**ê°œì„  ë°©ì•ˆ:**
```python
# 1. ê²Œì„ ì¢…ë£Œ ì‹œ ê²½í—˜ ì €ì¥
def end_episode(self):
    # ... ê¸°ì¡´ í•™ìŠµ ...

    # ê²½í—˜ ë°ì´í„° ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_path = f"local_training/data/buffer/exp_{timestamp}.npz"
    self.save_experience_data(exp_path)

# 2. Background Learnerì™€ ì—°ë™
# background_parallel_learner.pyì—ì„œ:
def process_buffer_file(self, file_path):
    data = np.load(file_path)
    experiences = [{
        'states': data['states'],
        'actions': data['actions'],
        'rewards': data['rewards']
    }]
    result = self.rl_agent.train_from_batch(experiences)
    # ... ì•„ì¹´ì´ë¹™ ...
```

---

## ğŸ¯ ìš°ì„ ìˆœìœ„ë³„ ê°œì„  ê³¼ì œ

### High Priority (ì¦‰ì‹œ ìˆ˜ì • í•„ìš”)

1. **Epsilon-Greedy ì¶”ê°€**
   - í•™ìŠµ/ì¶”ë¡  ëª¨ë“œ êµ¬ë¶„
   - ì´ˆê¸° íƒí—˜ â†’ ì ì§„ì  í™œìš©
   - â†’ "Untrained" ë¬¸ì œ í•´ê²°

2. **Actor-Critic ì „í™˜**
   - Value Network ì¶”ê°€
   - Variance ê°ì†Œ
   - í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ

3. **ê²½í—˜ ì¬ì‚¬ìš© êµ¬í˜„**
   - Experience Replay Buffer
   - Batch Learning í™œì„±í™”
   - Sample Efficiency í–¥ìƒ

### Medium Priority (ì„±ëŠ¥ ê°œì„ )

4. **ë³´ìƒ ì •ê·œí™”**
   - Running Mean/Std ì ìš©
   - ë³´ìƒ í´ë¦¬í•‘

5. **ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ**
   - ë°°í¬ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨
   - ì„±ëŠ¥ ì¶”ì 

6. **Background Learning ì—°ë™**
   - ê²½í—˜ ë°ì´í„° ìë™ ì €ì¥
   - ì˜¤í”„ë¼ì¸ ë°°ì¹˜ í•™ìŠµ

### Low Priority (ìµœì í™”)

7. **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§**
   - Cosine Annealing
   - Step Decay

8. **ìƒíƒœ ì •ê·œí™” ê°œì„ **
   - StandardScaler ì ìš©
   - Feature Engineering

---

## ğŸ’¡ Quick Fix: Epsilon-Greedy êµ¬í˜„

ê°€ì¥ ì‹œê¸‰í•œ "Untrained" ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ìµœì†Œ ìˆ˜ì •ì•ˆ:

```python
# rl_agent.py ìˆ˜ì •
class RLAgent:
    def __init__(self, learning_rate=0.001, gamma=0.99, model_path=None):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        self.epsilon = 1.0  # ì´ˆê¸° íƒí—˜ë¥ 
        self.epsilon_min = 0.1  # ìµœì†Œ íƒí—˜ë¥ 
        self.epsilon_decay = 0.995  # ê°ì‡ ìœ¨

    def get_action(self, state, training=True):
        # ìƒíƒœ ì •ê·œí™”
        if len(state) < self.policy.input_dim:
            state = np.concatenate([state, np.zeros(self.policy.input_dim - len(state))])
        state = state[:self.policy.input_dim].astype(np.float32)

        probs, cache = self.policy.forward(state)

        # Epsilon-greedy ì „ëµ
        if training and np.random.rand() < self.epsilon:
            # íƒí—˜: ëœë¤ í–‰ë™
            action_idx = np.random.randint(len(probs))
        else:
            # í™œìš©: í•™ìŠµëœ ì •ì±… ì‚¬ìš©
            if training:
                action_idx = np.random.choice(len(probs), p=probs)
            else:
                # ì¶”ë¡  ëª¨ë“œ: greedy
                action_idx = np.argmax(probs)

        if training:
            self.states.append(state)
            self.actions.append(action_idx)
            self.caches.append(cache)

        return action_idx, self.action_labels[action_idx], float(probs[action_idx])

    def end_episode(self, final_reward=0.0):
        # ... ê¸°ì¡´ í•™ìŠµ ì½”ë“œ ...

        # Epsilon ê°ì‡ 
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

        return stats

    def is_trained(self):
        """í•™ìŠµ ì™„ë£Œ ì—¬ë¶€ íŒë‹¨"""
        return self.episode_count >= 50 and self.epsilon <= 0.2
```

```python
# wicked_zerg_bot_pro_impl.py ìˆ˜ì •
# Line 294-305 ë³€ê²½
try:
    from local_training.rl_agent import RLAgent
    initial_lr = self.adaptive_lr.get_current_lr() if self.adaptive_lr else 0.001
    self.rl_agent = RLAgent(learning_rate=initial_lr)

    # í•™ìŠµ ì™„ë£Œ ì—¬ë¶€ í™•ì¸
    if self.rl_agent.is_trained():
        print(f"[BOT] RL Agent initialized (Trained: {self.rl_agent.episode_count} episodes, Îµ={self.rl_agent.epsilon:.3f})")
    else:
        print(f"[BOT] RL Agent initialized (Training: {self.rl_agent.episode_count} episodes, needs {50-self.rl_agent.episode_count} more)")
except ImportError as e:
    print(f"[WARNING] RL Agent not available: {e}")
    self.rl_agent = None
```

---

## ğŸ“ˆ ì˜ˆìƒ íš¨ê³¼

### Quick Fix ì ìš© ì‹œ
- âœ… ì´ˆê¸° ëœë¤ í–‰ë™ â†’ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ í´ë°±
- âœ… 50 ê²Œì„ ì´í›„ ì ì§„ì ìœ¼ë¡œ RL í™œì„±í™”
- âœ… "Untrained" ë¬¸ì œ í•´ê²°
- âœ… ì‚¬ìš©ì ì‹ ë¢° íšŒë³µ

### ì „ì²´ ê°œì„  ì™„ë£Œ ì‹œ
- âœ… í•™ìŠµ ì†ë„ 3-5ë°° í–¥ìƒ (Experience Replay)
- âœ… ìˆ˜ë ´ ì•ˆì •ì„± í–¥ìƒ (Actor-Critic)
- âœ… ì„±ëŠ¥ í–¥ìƒ 20-30% (Reward Shaping + Normalization)
- âœ… ì‹¤ì‹œê°„ í•™ìŠµ ê°€ëŠ¥ (Background Learning)

---

## ğŸ”§ ë‹¤ìŒ ë‹¨ê³„

1. **Epsilon-Greedy êµ¬í˜„** (1ì‹œê°„)
2. **ëª¨ë¸ ê²€ì¦ ë¡œì§ ì¶”ê°€** (30ë¶„)
3. **ê²½í—˜ ë°ì´í„° ì €ì¥ í™œì„±í™”** (30ë¶„)
4. **í…ŒìŠ¤íŠ¸ ë° ê²€ì¦** (10+ ê²Œì„)

---

**ì‘ì„±ì ì½”ë©˜íŠ¸:**
í˜„ì¬ RLAgentëŠ” ì´ë¡ ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì–´ ìˆì§€ë§Œ, **ì‹¤ì „ ë°°í¬ë¥¼ ìœ„í•œ ì•ˆì „ì¥ì¹˜ê°€ ë¶€ì¡±**í•©ë‹ˆë‹¤. íŠ¹íˆ ì´ˆê¸° í•™ìŠµ ë‹¨ê³„ì—ì„œ ì™„ì „ ëœë¤ í–‰ë™ìœ¼ë¡œ ì¸í•´ ì‚¬ìš©ì ê²½í—˜ì´ ë‚˜ì˜ê³ , ì´ê²ƒì´ ë¹„í™œì„±í™” ì‚¬ìœ ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. Quick Fixë§Œ ì ìš©í•´ë„ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ê°œì„ ë©ë‹ˆë‹¤.
