# Background Training System - ê²€í†  ê²°ê³¼ ë° ê°œì„  ë°©ì•ˆ

## ë°œê²¬ëœ ë¬¸ì œì 

### âš ï¸ ë¬¸ì œ 1: ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ ë¶ˆì¼ì¹˜ (Critical)

**í˜„ìƒ:**
```python
# ì˜¨ë¼ì¸ í•™ìŠµ (1ê²Œì„ = 50 ìŠ¤í…)
for step in 50:
    backward()  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
update_weights(lr=0.001)  # 50ë²ˆ ëˆ„ì  í›„ ì—…ë°ì´íŠ¸

# ì˜¤í”„ë¼ì¸ í•™ìŠµ (10ê²Œì„ = 500 ìŠ¤í…)
for game in 10:
    for step in 50:
        backward()  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
update_weights(lr=0.001)  # 500ë²ˆ ëˆ„ì  í›„ ì—…ë°ì´íŠ¸
```

**ë¬¸ì œ:**
- ì˜¤í”„ë¼ì¸ í•™ìŠµì˜ ê·¸ë˜ë””ì–¸íŠ¸ê°€ **10ë°° ë” í¬ë‹¤**
- ê°™ì€ learning rateë¥¼ ì‚¬ìš©í•˜ë©´ ì—…ë°ì´íŠ¸ í­ì´ 10ë°°
- í•™ìŠµ ë¶ˆì•ˆì •, ë°œì‚° ê°€ëŠ¥ì„±

**ìœ„ì¹˜:** `local_training/rl_agent.py:359-361`

**í•´ê²° ë°©ì•ˆ:**

**ì˜µì…˜ A: Learning Rate ì¡°ì • (ê¶Œì¥)**
```python
# train_from_batch() ìˆ˜ì •
adjusted_lr = self.learning_rate / len(experiences)  # ë°°ì¹˜ í¬ê¸°ë¡œ ì¡°ì •
self.policy.update_weights(adjusted_lr)
```

**ì˜µì…˜ B: ê° ê²Œì„ë§ˆë‹¤ ì—…ë°ì´íŠ¸**
```python
for exp in experiences:
    # ... ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ...
    self.policy.update_weights(self.learning_rate)  # ê° ê²Œì„ë§ˆë‹¤ ì—…ë°ì´íŠ¸
```

**ì˜µì…˜ C: ê·¸ë˜ë””ì–¸íŠ¸ í‰ê· í™”**
```python
# PolicyNetwork.update_weights() ìˆ˜ì •
# ëˆ„ì ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ step ìˆ˜ë¡œ ë‚˜ëˆ„ê¸°
self.W1 -= learning_rate * (self.dW1 / step_count)
```

---

### âš ï¸ ë¬¸ì œ 2: Off-Policy í•™ìŠµ (Medium)

**í˜„ìƒ:**
```python
# ê³¼ê±°ì— ì €ì¥ëœ ê²½í—˜ ë°ì´í„°
states = [s1, s2, s3, ...]
actions = [a1, a2, a3, ...]  # ê³¼ê±° ì •ì±…ì´ ì„ íƒí•œ ì•¡ì…˜

# í˜„ì¬ í•™ìŠµ ì‹œ
for state, action in zip(states, actions):
    probs, cache = self.policy.forward(state)  # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í™•ë¥  ê³„ì‚°
    self.policy.backward(cache, action, ...)  # ê³¼ê±° ì•¡ì…˜ìœ¼ë¡œ í•™ìŠµ
```

**ë¬¸ì œ:**
- `actions`ëŠ” **ê³¼ê±° ëª¨ë¸**(ê²Œì„ ë‹¹ì‹œ)ì´ ì„ íƒí•œ ì•¡ì…˜
- `probs`ëŠ” **í˜„ì¬ ëª¨ë¸**(í•™ìŠµ ì‹œì )ì´ ê³„ì‚°í•œ í™•ë¥ 
- ëª¨ë¸ì´ ì—¬ëŸ¬ ë²ˆ ì—…ë°ì´íŠ¸ë˜ì–´ ì •ì±…ì´ ë‹¬ë¼ì¡Œì„ ìˆ˜ ìˆìŒ
- REINFORCEëŠ” On-Policy ì•Œê³ ë¦¬ì¦˜ì¸ë° Off-Policyë¡œ ì‚¬ìš©ë¨

**ìœ„ì¹˜:** `local_training/rl_agent.py:346-351`

**ì™œ ì´ë ‡ê²Œ êµ¬í˜„ëë‚˜?**
- ê²½í—˜ ë°ì´í„°ì— `cache` (forward ê²°ê³¼)ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ
- íŒŒì¼ í¬ê¸° ì ˆì•½ + ëª¨ë¸ êµ¬ì¡° ë³€ê²½ ì‹œ í˜¸í™˜ì„±
- ì˜¨ë¼ì¸ í•™ìŠµì€ `self.caches`ì— ì €ì¥í•˜ì—¬ On-Policy ìœ ì§€

**í•´ê²° ë°©ì•ˆ:**

**ì˜µì…˜ A: Importance Sampling (ì´ë¡ ì ìœ¼ë¡œ ì˜¬ë°”ë¦„)**
```python
# í–‰ë™ í™•ë¥  ë¹„ìœ¨ë¡œ ë³´ì •
old_prob = ...  # ê³¼ê±° ì •ì±…ì˜ í™•ë¥  (ì €ì¥ í•„ìš”)
new_prob = probs[action]
importance_ratio = new_prob / (old_prob + 1e-9)
adjusted_advantage = advantage * importance_ratio
self.policy.backward(cache, action, adjusted_advantage)
```
â†’ í•˜ì§€ë§Œ `old_prob`ë¥¼ ì €ì¥í•´ì•¼ í•¨ (íŒŒì¼ í¬ê¸° ì¦ê°€)

**ì˜µì…˜ B: ìµœê·¼ ê²½í—˜ë§Œ ì‚¬ìš© (ì‹¤ìš©ì )**
```python
# ì˜¤ë˜ëœ íŒŒì¼ ìë™ ì‚­ì œ
if file_age > MAX_AGE:  # ì˜ˆ: 1ì‹œê°„
    skip_file
```
â†’ ëª¨ë¸ì´ í¬ê²Œ ë³€í•˜ê¸° ì „ì˜ ê²½í—˜ë§Œ ì‚¬ìš©

**ì˜µì…˜ C: Off-Policy ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì „í™˜**
- PPO (Proximal Policy Optimization)
- SAC (Soft Actor-Critic)
â†’ ëŒ€ëŒ€ì ì¸ ë¦¬íŒ©í† ë§ í•„ìš”

**ì˜µì…˜ D: í˜„ì¬ ìƒíƒœ ìœ ì§€ + Learning Rate ë‚®ì¶¤ (íƒ€í˜‘ì•ˆ)**
```python
# ë°°ì¹˜ í•™ìŠµ ì‹œ ë” ë‚®ì€ learning rate ì‚¬ìš©
batch_lr = self.learning_rate * 0.1  # 10% ìˆ˜ì¤€
self.policy.update_weights(batch_lr)
```

---

### âš ï¸ ë¬¸ì œ 3: ë™ì‹œì„± - Lost Update (Medium)

**í˜„ìƒ:**
```
ì‹œê°„ T0:
  ëª¨ë¸ ìƒíƒœ = V1 (ê°€ì¤‘ì¹˜ W1, episode_count=10)

ì‹œê°„ T1:
  [ë©”ì¸ ìŠ¤ë ˆë“œ] ëª¨ë¸ V1 ë¡œë“œ â†’ ì˜¨ë¼ì¸ í•™ìŠµ
  [ë°±ê·¸ë¼ìš´ë“œ] ëª¨ë¸ V1 ë¡œë“œ â†’ ë°°ì¹˜ í•™ìŠµ

ì‹œê°„ T2:
  [ë©”ì¸ ìŠ¤ë ˆë“œ] ëª¨ë¸ V2 ì €ì¥ (W2, episode_count=11)

ì‹œê°„ T3:
  [ë°±ê·¸ë¼ìš´ë“œ] ëª¨ë¸ V3 ì €ì¥ (W3, episode_count=11)  â† V2 ë®ì–´ì”€!
```

**ë¬¸ì œ:**
- ë‘ ìŠ¤ë ˆë“œê°€ ë™ì‹œì— ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ë©´ í•œìª½ì˜ ë³€ê²½ì‚¬í•­ì´ ì†ì‹¤
- `episode_count`ê°€ ë¶€ì •í™•í•´ì§
- í•™ìŠµ ì§„í–‰ ìƒí™© ì¶”ì  ë¶ˆê°€

**ìœ„ì¹˜:**
- `wicked_zerg_bot_pro_impl.py:405-409` (ë©”ì¸ ìŠ¤ë ˆë“œ)
- `background_parallel_learner.py:188-193` (ë°±ê·¸ë¼ìš´ë“œ)

**í˜„ì¬ ì™„í™” ë°©ë²•:**
- Atomic Write (`.tmp` â†’ `replace()`) ì‚¬ìš©
- íŒŒì¼ ì†ìƒì€ ë°©ì§€ë¨
- í•˜ì§€ë§Œ Lost UpdateëŠ” ì—¬ì „íˆ ë°œìƒ ê°€ëŠ¥

**í•´ê²° ë°©ì•ˆ:**

**ì˜µì…˜ A: íŒŒì¼ ì ê¸ˆ (File Locking)**
```python
import fcntl  # Unix
import msvcrt  # Windows

def save_model_with_lock(self, path):
    with open(lock_file, 'w') as lock:
        fcntl.flock(lock, fcntl.LOCK_EX)  # ë°°íƒ€ì  ì ê¸ˆ
        # ëª¨ë¸ ì €ì¥
        fcntl.flock(lock, fcntl.LOCK_UN)
```
â†’ í”Œë«í¼ ì˜ì¡´ì , ë³µì¡í•¨

**ì˜µì…˜ A: íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ë³‘í•© (ê¶Œì¥)**
```python
# ëª¨ë¸ íŒŒì¼ì— timestamp ì¶”ê°€
np.savez(
    path,
    W1=..., b1=...,
    timestamp=time.time(),
    episode_count=...
)

# ë¡œë“œ ì‹œ ìµœì‹  íƒ€ì„ìŠ¤íƒ¬í”„ í™•ì¸
if loaded_timestamp > self.last_timestamp:
    # ìµœì‹  ëª¨ë¸ ì‚¬ìš©
```

**ì˜µì…˜ C: ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ë¹„í™œì„±í™” ì‹œê°„ ì„¤ì •**
```python
# ê²Œì„ ì§„í–‰ ì¤‘ì—ëŠ” ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì¼ì‹œ ì¤‘ì§€
if game_in_progress:
    background_learner.pause()
```

**ì˜µì…˜ D: ë³„ë„ ëª¨ë¸ íŒŒì¼ ì‚¬ìš© (ê°„ë‹¨í•¨)**
```python
# ì˜¨ë¼ì¸ í•™ìŠµ: rl_agent_model_online.npz
# ì˜¤í”„ë¼ì¸ í•™ìŠµ: rl_agent_model_offline.npz
# ì •ê¸°ì ìœ¼ë¡œ ë³‘í•©
```

---

### âš ï¸ ë¬¸ì œ 4: Baseline ë¶ˆì¼ì¹˜ (Low)

**í˜„ìƒ:**
```python
# ì˜¨ë¼ì¸ í•™ìŠµ (end_episode)
self.baseline = 0.95 * self.baseline + 0.05 * avg_return  # ë² ì´ìŠ¤ë¼ì¸ ì—…ë°ì´íŠ¸

# ì˜¤í”„ë¼ì¸ í•™ìŠµ (train_from_batch)
advantages = returns  # Baseline ì‚¬ìš© ì•ˆ í•¨ (ì£¼ì„: "ë°°ì¹˜ë§ˆë‹¤ ë‹¬ë¼ì§€ë¯€ë¡œ ë‹¨ìˆœí™”")
```

**ë¬¸ì œ:**
- ì˜¨ë¼ì¸ í•™ìŠµì€ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ë¶„ì‚° ê°ì†Œ
- ì˜¤í”„ë¼ì¸ í•™ìŠµì€ ë² ì´ìŠ¤ë¼ì¸ ì—†ì´ í•™ìŠµ
- í•™ìŠµ íš¨ìœ¨ì„± ì°¨ì´, ë¶ˆì¼ì¹˜

**ìœ„ì¹˜:** `local_training/rl_agent.py:333-334`

**í•´ê²° ë°©ì•ˆ:**

**ì˜µì…˜ A: ë°°ì¹˜ í•™ìŠµì—ë„ ë² ì´ìŠ¤ë¼ì¸ ì‚¬ìš©**
```python
# ê° ê²Œì„ë§ˆë‹¤ ë² ì´ìŠ¤ë¼ì¸ ì ìš©
for exp in experiences:
    returns = calculate_returns(exp['rewards'])
    advantages = returns - self.baseline  # ë² ì´ìŠ¤ë¼ì¸ ì‚¬ìš©
    # í•™ìŠµ í›„ ë² ì´ìŠ¤ë¼ì¸ ì—…ë°ì´íŠ¸ (ì„ íƒì )
    # self.baseline = 0.95 * self.baseline + 0.05 * np.mean(returns)
```

**ì˜µì…˜ B: ë°°ì¹˜ ì „ìš© ë² ì´ìŠ¤ë¼ì¸**
```python
# ë°°ì¹˜ ë‚´ì—ì„œ í‰ê·  ê³„ì‚°
batch_returns = [calculate_returns(exp['rewards']) for exp in experiences]
batch_baseline = np.mean(batch_returns)
advantages = returns - batch_baseline
```

---

### âš ï¸ ë¬¸ì œ 5: ìƒíƒœ ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„± (Low)

**í˜„ìƒ:**
```python
# train_from_batch
state = states[i]  # (50,) ë˜ëŠ” ê°€ë³€ ê¸¸ì´
state_input = state[:self.policy.input_dim]  # ì²« 15ê°œë§Œ ì‚¬ìš©
probs, cache = self.policy.forward(state_input)
```

**ë¬¸ì œ:**
- `state`ì˜ ê¸¸ì´ê°€ `input_dim`ë³´ë‹¤ ì‘ìœ¼ë©´?
- `state[:15]`ê°€ (10,) ì´ë©´ forwardì—ì„œ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥

**ìœ„ì¹˜:** `local_training/rl_agent.py:347`

**í•´ê²° ë°©ì•ˆ:**

```python
# ì•ˆì „í•œ ì²˜ë¦¬
if len(state) < self.policy.input_dim:
    state_input = np.concatenate([state, np.zeros(self.policy.input_dim - len(state))])
else:
    state_input = state[:self.policy.input_dim]
state_input = state_input.astype(np.float32)
```

---

## ìš°ì„ ìˆœìœ„ ë° ê¶Œì¥ ì¡°ì¹˜

### ğŸ”´ Critical (ì¦‰ì‹œ ìˆ˜ì • í•„ìš”)

**1. ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ ë¬¸ì œ**
- ì˜í–¥: í•™ìŠµ ë¶ˆì•ˆì •, ë°œì‚° ê°€ëŠ¥
- ê¶Œì¥: Learning Rate ì¡°ì • (ì˜µì…˜ A)
- êµ¬í˜„ ë‚œì´ë„: ì‰¬ì›€ (1ì¤„)

```python
# train_from_batch() ìˆ˜ì •
adjusted_lr = self.learning_rate / max(len(experiences), 1)
self.policy.update_weights(adjusted_lr)
```

### ğŸŸ¡ Important (ê³§ ìˆ˜ì • ê¶Œì¥)

**2. Off-Policy ë¬¸ì œ**
- ì˜í–¥: í•™ìŠµ íš¨ìœ¨ ì €í•˜, ë¶ˆì•ˆì •
- ê¶Œì¥: ìµœê·¼ ê²½í—˜ë§Œ ì‚¬ìš© + Learning Rate ë‚®ì¶¤ (ì˜µì…˜ B+D)
- êµ¬í˜„ ë‚œì´ë„: ì¤‘ê°„

```python
# íŒŒì¼ ë‚˜ì´ ì²´í¬
MAX_FILE_AGE = 3600  # 1ì‹œê°„
current_time = time.time()
for file_path in files:
    file_age = current_time - file_path.stat().st_mtime
    if file_age < MAX_FILE_AGE:
        experiences.append(load_file(file_path))

# Learning rate ë‚®ì¶¤
batch_lr = self.learning_rate * 0.5  # 50% ìˆ˜ì¤€
```

**3. ë™ì‹œì„± ë¬¸ì œ**
- ì˜í–¥: í•™ìŠµ ì§„í–‰ ì†ì‹¤ ê°€ëŠ¥
- ê¶Œì¥: ë³„ë„ ëª¨ë¸ íŒŒì¼ ì‚¬ìš© (ì˜µì…˜ D)
- êµ¬í˜„ ë‚œì´ë„: ì¤‘ê°„

```python
# ì˜¨ë¼ì¸: rl_agent_model.npz (ë©”ì¸)
# ì˜¤í”„ë¼ì¸: rl_agent_model_batch.npz (ë°±ê·¸ë¼ìš´ë“œ)
# ì£¼ê¸°ì ìœ¼ë¡œ ë³‘í•©
```

### ğŸŸ¢ Nice to Have (ì„ íƒì )

**4. Baseline ì¼ì¹˜**
- ì˜í–¥: í•™ìŠµ íš¨ìœ¨ ì•½ê°„ í–¥ìƒ
- ê¶Œì¥: ë°°ì¹˜ í•™ìŠµì—ë„ ë² ì´ìŠ¤ë¼ì¸ ì‚¬ìš©
- êµ¬í˜„ ë‚œì´ë„: ì‰¬ì›€

**5. ìƒíƒœ ë²¡í„° ì•ˆì „ì„±**
- ì˜í–¥: ì—ëŸ¬ ë°©ì§€
- ê¶Œì¥: íŒ¨ë”© ì¶”ê°€
- êµ¬í˜„ ë‚œì´ë„: ì‰¬ì›€

---

## ê¶Œì¥ ìˆ˜ì • ë¡œë“œë§µ

### Phase 1: ì•ˆì •í™” (âœ… ì™„ë£Œ)
1. âœ… ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ ìˆ˜ì •
   - `train_from_batch()`ì—ì„œ ë°°ì¹˜ í¬ê¸°ë¡œ learning rate ì¡°ì •
   - `adjusted_lr = self.learning_rate / max(num_games, 1)`
2. âœ… ìƒíƒœ ë²¡í„° ì•ˆì „ì„± ì¶”ê°€
   - ì§§ì€ ìƒíƒœ ë²¡í„°ì— ëŒ€í•œ íŒ¨ë”© ì²˜ë¦¬
3. âœ… ì˜¤ë˜ëœ ê²½í—˜ í•„í„°ë§
   - `max_file_age` íŒŒë¼ë¯¸í„° ì¶”ê°€ (ê¸°ë³¸ê°’: 1ì‹œê°„)
   - Off-Policy ë¬¸ì œ ì™„í™”
4. âœ… ë°°ì¹˜ learning rate í†µê³„ ì¶”ê°€
   - `adjusted_lr` ë¡œê¹… ë° ë³´ê³ 
5. âœ… Baseline ì¼ì¹˜
   - ë°°ì¹˜ í•™ìŠµì—ì„œë„ ë°°ì¹˜ í‰ê· ì„ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ì‚¬ìš©

### Phase 2: ê³ ê¸‰ ê¸°ëŠ¥ (ì„ íƒì )
6. â¬œ ë™ì‹œì„± ë¬¸ì œ í•´ê²° (íŒŒì¼ ë¶„ë¦¬ ë˜ëŠ” ë³‘í•© ë¡œì§)
7. â¬œ Importance Sampling (ê³ ê¸‰)
8. â¬œ PPOë¡œ ì•Œê³ ë¦¬ì¦˜ ì—…ê·¸ë ˆì´ë“œ (ëŒ€ê·œëª¨)

---

## âœ… Phase 1 ê°œì„  ì™„ë£Œ ë‚´ìš©

### ìˆ˜ì •ëœ íŒŒì¼
1. **local_training/rl_agent.py**
   - `train_from_batch()` ë©”ì„œë“œ ê°œì„ 
   - ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ ë³´ì •
   - ìƒíƒœ ë²¡í„° ì•ˆì „ì„± ì²˜ë¦¬
   - ë² ì´ìŠ¤ë¼ì¸ ì‚¬ìš©

2. **tools/background_parallel_learner.py**
   - `max_file_age` íŒŒë¼ë¯¸í„° ì¶”ê°€
   - ì˜¤ë˜ëœ íŒŒì¼ ìë™ í•„í„°ë§ ë° ì•„ì¹´ì´ë¹™
   - í†µê³„ í•­ëª© ì¶”ê°€ (files_skipped_old, last_adjusted_lr)
   - ë³´ê³ ì„œ ë° ë¡œê·¸ì— ìƒˆ ì •ë³´ ë°˜ì˜

### ìƒˆë¡œìš´ ê¸°ëŠ¥
- **ìë™ ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ ì¡°ì •**: ë°°ì¹˜ í¬ê¸°ì— ë”°ë¼ learning rate ìë™ ì¡°ì •
- **Off-Policy ì™„í™”**: 1ì‹œê°„ ì´ìƒ ì˜¤ë˜ëœ íŒŒì¼ì€ ìë™ ê±´ë„ˆë›°ê¸°
- **í–¥ìƒëœ ëª¨ë‹ˆí„°ë§**: Adjusted LR, ê±´ë„ˆë›´ íŒŒì¼ ìˆ˜ ë“± ì¶”ê°€ í†µê³„

### ì˜ˆìƒ íš¨ê³¼
- í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
- Off-Policyë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ ê°ì†Œ
- ë” ì •í™•í•œ ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

---

## í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

ìˆ˜ì • í›„ ë‹¤ìŒì„ í™•ì¸:

- [ ] ì˜¨ë¼ì¸ í•™ìŠµì´ ì •ìƒ ì‘ë™í•˜ëŠ”ê°€?
- [ ] ì˜¤í”„ë¼ì¸ í•™ìŠµì´ ì •ìƒ ì‘ë™í•˜ëŠ”ê°€?
- [ ] Lossê°€ ì•ˆì •ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ”ê°€?
- [ ] ëª¨ë¸ íŒŒì¼ì´ ì†ìƒë˜ì§€ ì•ŠëŠ”ê°€?
- [ ] episode_countê°€ ì •í™•í•œê°€?
- [ ] ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ê°€ ì—†ëŠ”ê°€?
- [ ] ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ê°€ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ”ê°€?

---

## ê²°ë¡ 

í˜„ì¬ ì‹œìŠ¤í…œì€ **ê¸°ë³¸ì ìœ¼ë¡œ ì‘ë™í•˜ì§€ë§Œ, ìµœì í™”ë˜ì§€ ì•Šì€ ìƒíƒœ**ì…ë‹ˆë‹¤.

**ì¥ì :**
- âœ… ì˜¨ë¼ì¸ + ì˜¤í”„ë¼ì¸ í•™ìŠµ ì¡°í•© (ì¢‹ì€ ì•„ì´ë””ì–´)
- âœ… Atomic Writeë¡œ íŒŒì¼ ì†ìƒ ë°©ì§€
- âœ… ê²½í—˜ ë°ì´í„° ì•„ì¹´ì´ë¹™

**ë‹¨ì :**
- âŒ ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ ë¶ˆì¼ì¹˜ â†’ í•™ìŠµ ë¶ˆì•ˆì •
- âŒ Off-Policy í•™ìŠµ â†’ íš¨ìœ¨ ì €í•˜
- âŒ ë™ì‹œì„± ë¯¸í¡ â†’ ì—…ë°ì´íŠ¸ ì†ì‹¤ ê°€ëŠ¥

**Phase 1 ìˆ˜ì •ë§Œìœ¼ë¡œë„ ì•ˆì •ì„±ì´ í¬ê²Œ í–¥ìƒ**ë  ê²ƒì…ë‹ˆë‹¤.
