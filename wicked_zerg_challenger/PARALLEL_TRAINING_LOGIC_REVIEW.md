# 병렬 훈련 로직 정밀 검토 보고서 (Parallel Training Logic Review)

사용자가 요청한 `tools/background_parallel_learner.py` 및 관련 파일들을 분석하여, 병렬 훈련 시스템의 안정성, 동기화 방식, 잠재적 위험 요소를 검토했습니다.

## 1. 아키텍처 개요
현재 구현된 시스템은 **"비동기식 백그라운드 학습 (Asynchronous Background Learning)"** 구조입니다.

*   **생산자 (Producer):** `WickedZergBotProImpl` (게임 봇)
    *   게임이 끝날 때마다 경험 데이터(State, Action, Reward)를 `.npz` 파일로 `buffer/` 디렉토리에 저장합니다.
*   **소비자 (Consumer):** `BackgroundParallelLearner` (백그라운드 스레드)
    *   주기적으로 `buffer/` 디렉토리를 확인합니다.
    *   파일이 쌓이면 이를 읽어와서 `RLAgent`를 학습시킵니다.
    *   학습된 모델을 디스크에 다시 저장합니다.

## 2. 동기화 및 레이스 컨디션 (Race Conditions)
가장 중요한 "모델 읽기/쓰기 충돌" 문제를 어떻게 해결했는지 확인했습니다.

*   **방식:** **파일 시스템 기반 동기화 (File System-based Sync)**
*   **안정성:** ✅ **양호 (Atomic Write 사용)**
    *   `RLAgent.save_model()` 메서드는 임시 파일(`.tmp`)에 먼저 쓰고, 완료되면 원본 파일로 **이름 변경(Rename)**하는 원자적(Atomic) 방식을 사용합니다.
    *   덕분에 봇이 모델을 로드하는 순간에 "파일이 쓰이는 중이라 깨지는" 문제는 발생하지 않습니다.

## 3. 잠재적 위험 요소

### 3-1. Off-Policy 문제 (학습과 수행의 괴리)
*   **현상:** 봇이 게임을 하는 동안 백그라운드에서 학습이 진행됩니다. 봇은 게임 시작 시점의 모델(구버전)을 가지고 게임을 끝까지 진행하지만, 그 사이에 학습기는 모델을 업데이트할 수 있습니다.
*   **영향:** REINFORCE 알고리즘은 본래 On-Policy(행동한 정책으로만 학습) 방식입니다. 정책이 너무 달라지면 학습 효율이 떨어질 수 있습니다.
*   **현재 대책:** `max_file_age` 설정을 통해 너무 오래된 데이터(1시간 이상)는 학습에서 배제하는 안전장치가 있어 큰 문제는 아닐 것으로 판단됩니다.

### 3-2. I/O 오버헤드
*   **현상:** 매 배치 학습마다 모델을 디스크에서 다시 로드하고(`reload`), 학습 후 다시 저장합니다.
*   **영향:** 디스크 I/O가 빈번하게 발생합니다. SSD가 아닌 경우 성능 저하가 있을 수 있으나, 현재 규모에서는 허용 가능한 수준입니다.

## 4. 종합 의견

*   **구현 상태:** **매우 견고함 (Robust)**
    *   복잡한 뮤텍스(Lock)나 세마포어를 쓰지 않고, 파일 시스템을 큐(Queue)처럼 사용하는 방식은 파이썬의 GIL(Global Interpreter Lock) 제약을 우회하고 구현 복잡도를 낮추는 현명한 선택입니다.
*   **데드락(Deadlock) 위험:** **없음**
    *   쓰레드 간에 락(Lock)을 공유하지 않으므로 교착 상태가 발생할 구조가 아닙니다.

## 5. 개선 권장 사항
1.  **배치 크기 증대:** 현재 10개의 게임 파일을 한 번에 처리합니다. 학습 안정성을 위해 이 배치를 20~50 정도로 늘리는 것을 고려해볼 수 있습니다.
2.  **데이터 정리:** `archive/` 폴더에 처리된 데이터가 무한히 쌓입니다. 디스크 용량을 위해 `tools/cleanup_artifacts.py` 등을 주기적으로 실행하여 오래된 아카이브를 삭제해야 합니다.
